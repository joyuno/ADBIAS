#!/usr/bin/env python3
# ================================================================
#  0. ì›ë³¸ import / ì„¤ì • / í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ)
# ================================================================
import pandas as pd
import os, time, json, requests, argparse, re

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° ê²½ë¡œ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ESSAY_FILE_PATH   = './data/asap_data/asap_2.csv'
RUBRICS_PATH      = './data/rubrics/2_rubrics.txt'
PROMPT_PATH       = './data/rubrics/2_prompt.txt'
# OUTPUT_JSON_PATH  = './results/results2/output_e2_results_claude.json'
# OUTPUT_JSON_PATH  = './results/results2/output_e2_results_gemini_merged_fixed_full.json'
# OUTPUT_JSON_PATH  = './results/results2/output_e2_results_gpt.json'
OUTPUT_JSON_PATH  = './results/results2/output_e2_results_llama.json'

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# OpenRouter í™˜ê²½ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
API_CALL_DELAY    = 1
OPENROUTER_API_URL = "https://openrouter.ai/api/v1/chat/completions"
OPENROUTER_API_KEY = 'ë‹¹ì‹ ì˜ í‚¤ë¥¼ ë„£ìœ¼ì‹œì˜¤!!'
# LLM_MODEL = "anthropic/claude-3.5-sonnet" 
# LLM_MODEL = "google/gemini-2.5-flash-preview" 
# LLM_MODEL = "openai/gpt-4o" 
LLM_MODEL = "meta-llama/llama-4-maverick" 

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# (ë£¨ë¸Œë¦­Â·í”„ë¡¬í”„íŠ¸ ë¡œë”© / build_system_prompt / build_user_prompt
#  evaluate_essay_openrouter í•¨ìˆ˜ëŠ” ê·¸ëŒ€ë¡œ, ìƒëµ ì—†ì´ ìœ ì§€)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_rubric():
    with open(RUBRICS_PATH, 'r', encoding='utf-8') as f:
        rubric_content = f.read().strip()
    with open(PROMPT_PATH, 'r', encoding='utf-8') as f:
        prompt_text = f.read().strip()
    return rubric_content, prompt_text

def build_system_prompt(rubric_content):
    return f"""
You are a rater for essays written by students from grades 10.

Use the following scoring criteria when evaluating the essays:
{rubric_content}
""".strip()

def build_user_prompt(essay_id, essay_set, prompt_text, essay_text):
    sample_format = f'''
{{
  "essay_id": "{essay_id}",
  "essay_set": "{essay_set}",
  "scores": {{
    "content": {{"rationale": "Your rationale here.", "score": 0}},
    "organization": {{"rationale": "Your rationale here.", "score": 0}},
    "word choice": {{"rationale": "Your rationale here.", "score": 0}},
    "sentence fluency": {{"rationale": "Your rationale here.", "score": 0}},
    "conventions": {{"rationale": "Your rationale here.", "score": 0}}
  }}
}}
'''
    return f"""
You will be sent with an essay prompt and a student's essay response.

First, write a justification for each trait: Content, Organization, Word choice, Sentence fluency, and Conventions.
Then, assign a score from 1 to 6 for each trait.

Ratings are based on the rubric guidelines provided in the system message.

Notes: Remember that these essays are written by grade 10 students.
Avoid interpreting the rubric too strictly; consider what is developmentally appropriate for grade 10 students.

You should respond with your justification and give your score in the subsequent Python dictionary-like structure:
- No triple backticks (```)
- No "json" label
- No markdown formatting

Follow exactly this structure:
{sample_format}

Essay Prompt:
{prompt_text}

Student Essay:
{essay_text}
""".strip()

def evaluate_essay_openrouter(essay_id, essay_set, essay_text,
                              prompt_text, rubric_content, failed_set=None):
    system_prompt = build_system_prompt(rubric_content)
    user_prompt   = build_user_prompt(essay_id, essay_set, prompt_text, essay_text)

    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json",
    }
    payload = {
        "model": LLM_MODEL,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "temperature": 0,
        "max_tokens": 2048
    }

    try:
        resp = requests.post(OPENROUTER_API_URL, headers=headers, json=payload, timeout=60)
        resp.raise_for_status()
        result_text = resp.json()['choices'][0]['message']['content'].strip()
        parsed_result = json.loads(result_text)
        time.sleep(API_CALL_DELAY)
        return parsed_result
    except Exception as e:
        print(f"âš ï¸ ì˜¤ë¥˜(ID {essay_id}): {e}")
        if failed_set is not None:
            failed_set.add(str(essay_id))
        return {
            "essay_id": essay_id,
            "essay_set": essay_set,
            "scores": {
                trait: {"score": 0, "rationale": f"Error: {e}"}
                for trait in ['content', 'organization', 'word choice',
                              'sentence fluency', 'conventions']
            }
        }

# ================================================================
# 1. ì‹¤íŒ¨ ID ì¶”ì¶œ (score==0 & 'Error:' í¬í•¨ ê¸°ì¤€)
# ================================================================
def extract_failed_ids(json_path):
    with open(json_path, encoding='utf-8') as f:
        data = json.load(f)
    fail_ids = {
        str(item["essay_id"])
        for item in data
        for v in item["scores"].values()
        if v["score"] == 0 and "Error" in v["rationale"]
    }
    print(f"[+] ê¸°ì¡´ ê²°ê³¼ì—ì„œ ì‹¤íŒ¨ ID {len(fail_ids)}ê°œ ê°ì§€")
    return fail_ids

# ================================================================
# 2. ë³‘í•© í•¨ìˆ˜ (ì •ë ¬ í‚¤ ì•ˆì „í™”)  â† íŒ¨ì¹˜
# ================================================================
def merge_results(orig_json_path, retry_json_path, merged_path):
    with open(orig_json_path, encoding='utf-8') as f:
        orig  = {str(o["essay_id"]): o for o in json.load(f)}
    with open(retry_json_path, encoding='utf-8') as f:
        retry = json.load(f)
    for item in retry:
        orig[str(item["essay_id"])] = item

    def sort_key(x):
        eid = str(x["essay_id"])
        return (0, int(eid)) if eid.isdigit() else (1, eid.lower())

    merged = sorted(orig.values(), key=sort_key)
    with open(merged_path, 'w', encoding='utf-8') as f:
        json.dump(merged, f, indent=2, ensure_ascii=False)
    print(f"âœ… ë³‘í•© ì™„ë£Œ â†’ {merged_path}")

# ================================================================
# 3. ì¬ì‹œë„ ëª¨ë“œ
# ================================================================
def retry_mode():
    RETRY_JSON  = OUTPUT_JSON_PATH.replace(".json", "_retry.json")
    MERGED_JSON = OUTPUT_JSON_PATH.replace(".json", "_merged.json")

    fail_ids = extract_failed_ids(OUTPUT_JSON_PATH)
    if not fail_ids:
        print("ğŸ‰ ì‹¤íŒ¨ IDê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œ.")
        return

    df = pd.read_csv(ESSAY_FILE_PATH, usecols=['essay_id', 'essay_set', 'essay'])
    retry_df = df[df["essay_id"].astype(str).isin(fail_ids)]
    print(f"[+] ì¬í‰ê°€ ëŒ€ìƒ {len(retry_df)}ê°œ")

    rubric_content, prompt_text = load_rubric()
    retry_results, still_fail = [], set()

    for _, row in retry_df.iterrows():
        res = evaluate_essay_openrouter(
            row['essay_id'], row['essay_set'], row['essay'],
            prompt_text, rubric_content, failed_set=still_fail
        )
        retry_results.append(res)

    os.makedirs(os.path.dirname(RETRY_JSON), exist_ok=True)
    with open(RETRY_JSON, 'w', encoding='utf-8') as f:
        json.dump(retry_results, f, indent=2, ensure_ascii=False)
    print(f"[âˆš] ì¬í‰ê°€ ê²°ê³¼ ì €ì¥ â†’ {RETRY_JSON}")

    merge_results(OUTPUT_JSON_PATH, RETRY_JSON, MERGED_JSON)

    with open("failed_ids_claude.json", 'w') as f:
        json.dump(sorted(still_fail), f, indent=2)
    print(f"[+] ì—¬ì „íˆ ì‹¤íŒ¨ {len(still_fail)}ê°œ ê¸°ë¡ ì™„ë£Œ")

# ================================================================
# 4. ë©”ì¸ ì‹¤í–‰ë¶€
# ================================================================
if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--retry", action="store_true",
                        help="ê¸°ì¡´ JSONì˜ ì˜¤ë¥˜ ë ˆì½”ë“œë§Œ ì¬í‰ê°€")
    args = parser.parse_args()

    if args.retry:
        retry_mode()
        exit()

    # â”€â”€â”€ 1ì°¨ ì „ì²´ í‰ê°€ (ê¸°ì¡´ ì½”ë“œ) â”€â”€â”€
    print(f"ì—ì„¸ì´ íŒŒì¼ ë¡œë”© ì¤‘: {ESSAY_FILE_PATH}")
    try:
        df = pd.read_csv(ESSAY_FILE_PATH, usecols=['essay_id', 'essay_set', 'essay'])
        print(f"ì´ {len(df)}ê°œì˜ ì—ì„¸ì´ ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"âŒ ì—ì„¸ì´ íŒŒì¼ ë¡œë”© ì‹¤íŒ¨: {e}")
        exit()

    rubric_content, prompt_text = load_rubric()
    all_results, processed_count = [], 0

    for _, row in df.iterrows():
        processed_count += 1
        print(f"\n--- ì—ì„¸ì´ {processed_count}/{len(df)} í‰ê°€ ì‹œì‘ "
              f"(ID: {row['essay_id']}, Set: {row['essay_set']}) ---")
        res = evaluate_essay_openrouter(
            row['essay_id'], row['essay_set'], row['essay'],
            prompt_text, rubric_content
        )
        all_results.append(res)

    os.makedirs(os.path.dirname(OUTPUT_JSON_PATH), exist_ok=True)
    with open(OUTPUT_JSON_PATH, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    print(f"âœ… í‰ê°€ ê²°ê³¼ê°€ '{OUTPUT_JSON_PATH}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
